---
layout: mypost
title: 06-协程
categories: [Python]
---

## asyncio

- 同步版本

```python
import time

def crawl_page(url):
    print(f"crawling {url}")
    sleep_time = int(url.split('_')[-1])
    time.sleep(sleep_time)
    print(f"ok {url}")

def main(urls):
    for url in urls:
        crawl_page(url)

%time main(['url_1', 'url_2', 'url_3', 'url_4', 'url_5', 'url_6', 'url_7'])

# ----输出----

crawling url_1
ok url_1
crawling url_2
ok url_2
crawling url_3
ok url_3
crawling url_4
ok url_4
crawling url_5
ok url_5
crawling url_6
ok url_6
crawling url_7
ok url_7
CPU times: user 6.96 ms, sys: 2.53 ms, total: 9.49 ms

# main() 函数执行时，调取 crawl_page() 函数进行网络通信，经过若干秒等待后收到结果，然后执行下一个
```

- 异步版本

```python
import asyncio

async def crawl_page_async(url):
    print('crawling {}'.format(url))
    sleep_time = int(url.split('_')[-1])
    await asyncio.sleep(sleep_time)
    print('OK {}'.format(url))

async def main_async(urls):
    for url in urls:
        await crawl_page_async(url)

%time asyncio.run(main_async(['url_1', 'url_2', 'url_3', 'url_4', 'url_5', 'url_6', 'url_7']))
print(main_async('')) # <coroutine object main_async at 0x7fdb53f0f940>
print(crawl_page_async('_1')) # <coroutine object crawl_page_async at 0x7fdb5502ae40>

# ----输出----

crawling url_1
OK url_1
crawling url_2
OK url_2
crawling url_3
OK url_3
crawling url_4
OK url_4
crawling url_5
OK url_5
crawling url_6
OK url_6
crawling url_7
OK url_7

# 执行下 会发现耗时还是28s，28s就对了，还记得上面所说的，await 是同步调用，因此， crawl_page_async(url) 在当前的调用结束之前，是不会触发下一次调用的。于是，这个代码效果就和上面完全一样了，相当于我们用异步接口写了个同步代码。
```

- async关键字声明异步函数，crawl_page_async和main_async都变成了异步函数，调用异步函数，就会得到一个协程对象
- 打印main_async和crawl_page_async，会输出<coroutine object crawl_page_async at 0xxxxxxxx>，这其实就是一个Python的协程对象，而并不会真正执行这个函数

## 协程的执行

- await调用
  - await执行的效果，和正常执行一样，程序会阻塞在这里，进入被调用的协程函数，执行完毕返回后再继续。代码中 await asyncio.sleep(sleep_time) 会在这里休息若干秒，await crawl_page_async(url) 则会执行 crawl_page() 函数。
- asyncio.create_task()创建任务
- asyncio.run()运行
  - asyncio.run(main())作为主程序的入口函数

### 任务Task

```python
import asyncio

async def crawl_page_async(url):
    print('crawling {}'.format(url))
    sleep_time = int(url.split('_')[-1])
    await asyncio.sleep(sleep_time)
    print('OK {}'.format(url))

async def main_async(urls):
    tasks = [asyncio.create_task(crawl_page_async(url)) for url in urls]
    for task in tasks:
        await task

%time asyncio.run(main_async(['url_1', 'url_2', 'url_3', 'url_4', 'url_5', 'url_6', 'url_7']))
print(main_async('')) # <coroutine object main_async at 0x7fdb53f0f940>
print(crawl_page_async('_1')) # <coroutine object crawl_page_async at 0x7fdb5502ae40>

# ----输出----

crawling url_1
crawling url_2
crawling url_3
crawling url_4
crawling url_5
crawling url_6
crawling url_7
OK url_1
OK url_2
OK url_3
OK url_4
OK url_5
OK url_6
OK url_7
<coroutine object main_async at 0x7fdb5509ca40>
<coroutine object crawl_page_async at 0x7fdb55095140>
```

- 当有了协程对象后，可以通过asyncio.create_task来创建任务，任务创建后很快会被调度执行，代码也不会阻塞，所以，我们要等所有任务都结束才行，用for task in tasks: await task 即可。
- 对于执行 tasks，还有另一种做法

```python
import asyncio

async def crawl_page_async(url):
    print('crawling {}'.format(url))
    sleep_time = int(url.split('_')[-1])
    await asyncio.sleep(sleep_time)
    print('OK {}'.format(url))

async def main_async(urls):
    tasks = [asyncio.create_task(crawl_page_async(url)) for url in urls]
    await asyncio.gather(*tasks)

%time asyncio.run(main_async(['url_1', 'url_2', 'url_3', 'url_4', 'url_5', 'url_6', 'url_7']))
print(main_async('')) # <coroutine object main_async at 0x7fdb53f0f940>
print(crawl_page_async('_1')) # <coroutine object crawl_page_async at 0x7fdb5502ae40>

# ----输出----

crawling url_2
crawling url_3
crawling url_4
crawling url_5
crawling url_6
crawling url_7
OK url_1
OK url_2
OK url_3
OK url_4
OK url_5
OK url_6
OK url_7
<coroutine object main_async at 0x7fdb55095140>
<coroutine object crawl_page_async at 0x7fdb55095140>
```

- *tasks 解包列表，将列表变成了函数的参数；与之对应的是， ** dict 将字典变成了函数的参数。

## 实现原理

```python

import asyncio

async def worker_1():
    print('worker_1 start')
    await asyncio.sleep(1)
    print('worker_1 done')

async def worker_2():
    print('worker_2 start')
    await asyncio.sleep(2)
    print('worker_2 done')

async def main():
    print('before await')
    await worker_1()
    print('awaited worker_1')
    await worker_2()
    print('awaited worker_2')

%time asyncio.run(main())

########## 输出 ##########

before await
worker_1 start
worker_1 done
awaited worker_1
worker_2 start
worker_2 done
awaited worker_2
```

```python

import asyncio

async def worker_1():
    print('worker_1 start')
    await asyncio.sleep(1)
    print('worker_1 done')

async def worker_2():
    print('worker_2 start')
    await asyncio.sleep(2)
    print('worker_2 done')

async def main():
    task1 = asyncio.create_task(worker_1())
    task2 = asyncio.create_task(worker_2())
    print('before await')
    await task1
    print('awaited worker_1')
    await task2
    print('awaited worker_2')

%time asyncio.run(main())

########## 输出 ##########

before await
worker_1 start
worker_2 start
worker_1 done
awaited worker_1
worker_2 done
awaited worker_2
```
- 代码解析
  - await main() 等价于 asyncio.run(main())，程序进入 main，事件循环开始
  - task1和task2任务被创建，进入事件循环并执行，运行到 print('before await'),输出`before await`
  - await task1 执行，用户从当前的主任务中切出，事件调度器开始调度worker_1
  - worker_1 开始运行，运行 print('worker_1 start')，输出`worker_1 start`，然后运行到 await asyncio.sleep(1)，从当前任务切出，事件调度器开始调度wworker_2
  - worker_2 开始运行，运行 print('worker_2 start')，输出`worker_2 start`，然后运行 await asyncio.sleep(2) 从当前任务切出；
  - 以上所有事件的运行时间，都应该在 1ms 到 10ms 之间，甚至可能更短，事件调度器从这个时候开始暂停调度；
  - 1秒钟之后，worker_1的sleep完成，事件调度器将控制权重新传给task1，输出`worker_1 done`，task1完成了任务，从事件循环中退出；
  - await task1完成。事件调度器将控制权传给主任务，输出`awaited worker_1`, 返回到 worker_1 继续等待
  - 两秒钟后，worker_2的sleep结束，事件调度器将控制权重新交给task_2,输出 'worker_2 done'，task_2 完成任务，从事件循环中退出；
  - 主任务输出 'awaited worker_2'，协程全任务结束，事件循环结束。

- 给某些协程任务限定运行时间，一旦超时就取消，协程错误处理

```python

import asyncio

async def worker_1():
    await asyncio.sleep(1)
    return 1

async def worker_2():
    await asyncio.sleep(2)
    return 2 / 0

async def worker_3():
    await asyncio.sleep(3)
    return 3

async def main():
    task_1 = asyncio.create_task(worker_1())
    task_2 = asyncio.create_task(worker_2())
    task_3 = asyncio.create_task(worker_3())

    await asyncio.sleep(2)
    task_3.cancel()

    res = await asyncio.gather(task_1, task_2, task_3, return_exceptions=True)
    print(res)

%time asyncio.run(main())

########## 输出 ##########

[1, ZeroDivisionError('division by zero'), CancelledError()]
Wall time: 2 s

# worker_1 正常运行，worker_2 运行中出现错误，worker_3 执行时间过长被我们 cancel 掉了，这些信息会全部体现在最终的返回结果 res 中。不过要注意return_exceptions=True这行代码。
# 如果不设置这个参数，错误就会完整地 throw 到我们这个执行层，从而需要 try except 来捕捉，这也就意味着其他还没被执行的任务会被全部取消掉。为了避免这个局面，我们将 return_exceptions 设置为 True 即可。
```

- 协程来实现一个经典的生产者消费者模型吧

```python

import asyncio
import random

async def consumer(queue, id):
    while True:
        val = await queue.get()
        print('{} get a val: {}'.format(id, val))
        await asyncio.sleep(1)

async def producer(queue, id):
    for i in range(5):
        val = random.randint(1, 10)
        await queue.put(val)
        print('{} put a val: {}'.format(id, val))
        await asyncio.sleep(1)

async def main():
    queue = asyncio.Queue()

    consumer_1 = asyncio.create_task(consumer(queue, 'consumer_1'))
    consumer_2 = asyncio.create_task(consumer(queue, 'consumer_2'))

    producer_1 = asyncio.create_task(producer(queue, 'producer_1'))
    producer_2 = asyncio.create_task(producer(queue, 'producer_2'))

    await asyncio.sleep(10)
    consumer_1.cancel()
    consumer_2.cancel()
    
    await asyncio.gather(consumer_1, consumer_2, producer_1, producer_2, return_exceptions=True)

%time asyncio.run(main())

########## 输出 ##########

producer_1 put a val: 5
producer_2 put a val: 3
consumer_1 get a val: 5
consumer_2 get a val: 3
producer_1 put a val: 1
producer_2 put a val: 3
consumer_2 get a val: 1
consumer_1 get a val: 3
producer_1 put a val: 6
producer_2 put a val: 10
consumer_1 get a val: 6
consumer_2 get a val: 10
producer_1 put a val: 4
producer_2 put a val: 5
consumer_2 get a val: 4
consumer_1 get a val: 5
producer_1 put a val: 2
producer_2 put a val: 8
consumer_1 get a val: 2
consumer_2 get a val: 8
Wall time: 10 s
```

## 实例

- 豆瓣电影今日推荐爬虫

```python
# 同步版本代码

import requests
from bs4 import BeautifulSoup

def main():
    headers = {
    'Host':'movie.douban.com',
    'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36',
    'Cookie':'bid=F1l4ghtrj_4; ap_v=0,6.0; _pk_id.100001.4cf6=7931be2691d5364a.1667973825.1.1667973825.1667973825.; _pk_ses.100001.4cf6=*; __utma=30149280.1548109094.1667973825.1667973825.1667973825.1; __utmb=30149280.0.10.1667973825; __utmc=30149280; __utmz=30149280.1667973825.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none); __utma=223695111.1889506471.1667973825.1667973825.1667973825.1; __utmb=223695111.0.10.1667973825; __utmc=223695111; __utmz=223695111.1667973825.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none)'
    }
    url = "https://movie.douban.com/cinema/later/beijing/"
    init_page = requests.get(url, headers=headers).content
    init_soup = BeautifulSoup(init_page, 'lxml')

    all_movies = init_soup.find('div', id="showing-soon")
    for each_movie in all_movies.find_all('div', class_="item"):
        all_a_tag = each_movie.find_all('a')
        all_li_tag = each_movie.find_all('li')

        movie_name = all_a_tag[1].text
        url_to_fetch = all_a_tag[1]['href']
        movie_date = all_li_tag[0].text

        response_item = requests.get(url_to_fetch, headers=headers).content
        soup_item = BeautifulSoup(response_item, 'lxml')
        img_tag = soup_item.find('img')

        print('{} {} {}'.format(movie_name, movie_date, img_tag['src']))

%time main()

########## 输出 ##########

扫黑行动 11月11日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2883219876.jpg
陪你在全世界长大 11月11日 https://img2.doubanio.com/view/photo/s_ratio_poster/public/p2879590432.jpg
叫我郑先生 11月11日 https://img2.doubanio.com/view/photo/s_ratio_poster/public/p2882858722.jpg
你好，珠峰 11月11日 https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2882856388.jpg
我们正年轻 11月11日 https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2829300519.jpg
浏阳河上 11月11日 https://img2.doubanio.com/view/photo/s_ratio_poster/public/p2831086052.jpg
天之书 11月12日 https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2880250328.jpg
您好，北京 11月18日 https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2882109748.jpg
个十百千万 11月18日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2883212096.jpg
让这首歌作证 11月18日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2882862234.jpg
绑架游戏 12月23日 https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2869898539.jpg
龙马精神 12月31日 https://img3.doubanio.com/view/photo/s_ratio_poster/public/p2880320460.jpg
保你平安 12月31日 https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2882690157.jpg
透明侠侣 12月31日 https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2876027489.jpg
绝望主夫 12月31日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2882513134.jpg
CPU times: user 586 ms, sys: 22.9 ms, total: 609 ms
Wall time: 11.5 s
```

```python
# 异步版本代码

import asyncio
import aiohttp

from bs4 import BeautifulSoup

async def fetch_content(url):
    headers = {
    'Host':'movie.douban.com',
    'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36',
    'Cookie':'bid=F1l4ghtrj_4; ap_v=0,6.0; _pk_id.100001.4cf6=7931be2691d5364a.1667973825.1.1667973825.1667973825.; _pk_ses.100001.4cf6=*; __utma=30149280.1548109094.1667973825.1667973825.1667973825.1; __utmb=30149280.0.10.1667973825; __utmc=30149280; __utmz=30149280.1667973825.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none); __utma=223695111.1889506471.1667973825.1667973825.1667973825.1; __utmb=223695111.0.10.1667973825; __utmc=223695111; __utmz=223695111.1667973825.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none)'
    }
    async with aiohttp.ClientSession(
        headers=headers, connector=aiohttp.TCPConnector(ssl=False)
    ) as session:
        async with session.get(url) as response:
            return await response.text()

async def main():
    url = "https://movie.douban.com/cinema/later/hangzhou/"
    init_page = await fetch_content(url)
    init_soup = BeautifulSoup(init_page, 'lxml')

    movie_names, urls_to_fetch, movie_dates = [], [], []

    all_movies = init_soup.find('div', id="showing-soon")
    for each_movie in all_movies.find_all('div', class_="item"):
        all_a_tag = each_movie.find_all('a')
        all_li_tag = each_movie.find_all('li')

        movie_names.append(all_a_tag[1].text)
        urls_to_fetch.append(all_a_tag[1]['href'])
        movie_dates.append(all_li_tag[0].text)

    tasks = [fetch_content(url) for url in urls_to_fetch]
    pages = await asyncio.gather(*tasks)

    for movie_name, movie_date, page in zip(movie_names, movie_dates, pages):
        soup_item = BeautifulSoup(page, 'lxml')
        img_tag = soup_item.find('img')

        print('{} {} {}'.format(movie_name, movie_date, img_tag['src']))

%time asyncio.run(main())

########## 输出 ##########

扫黑行动 11月11日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2883219876.jpg
陪你在全世界长大 11月11日 https://img2.doubanio.com/view/photo/s_ratio_poster/public/p2879590432.jpg
叫我郑先生 11月11日 https://img2.doubanio.com/view/photo/s_ratio_poster/public/p2882858722.jpg
你好，珠峰 11月11日 https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2882856388.jpg
我们正年轻 11月11日 https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2829300519.jpg
浏阳河上 11月11日 https://img2.doubanio.com/view/photo/s_ratio_poster/public/p2831086052.jpg
天之书 11月12日 https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2880250328.jpg
您好，北京 11月18日 https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2882109748.jpg
个十百千万 11月18日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2883212096.jpg
让这首歌作证 11月18日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2882862234.jpg
绑架游戏 12月23日 https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2869898539.jpg
龙马精神 12月31日 https://img3.doubanio.com/view/photo/s_ratio_poster/public/p2880320460.jpg
保你平安 12月31日 https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2882690157.jpg
透明侠侣 12月31日 https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2876027489.jpg
绝望主夫 12月31日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2882513134.jpg
```

## Note

- 协程和多线程的区别，主要在于两点，一是协程为单线程；二是协程由用户决定，在哪些地方交出控制权，切换到下一个任务。
- 协程的写法更加简洁清晰，把 async / await 语法和 create_task 结合来用，对于中小级别的并发需求已经毫无压力。
- 写协程程序的时候，你的脑海中要有清晰的事件循环概念，知道程序在什么时候需要暂停、等待 I/O，什么时候需要一并执行到底。

- await
  - 开发者要提前知道一个任务的哪个环节会造成I/O阻塞，然后把这个环节的代码异步化处理，并且通过await来标识在任务的该环节中断该任务执行，从而去执行下一个事件循环任务。
  - 充分利用CPU资源，避免CPU等待I/O造成CPU资源白白浪费。当之前任务的那个环节的I/O完成后，线程可以从await获取返回值，然后继续执行没有完成的剩余代码。
  - 由上面分析可知，如果一个任务不涉及到网络或磁盘I/O这种耗时的操作，而只有CPU计算和内存I/O的操作时，协程并发的性能还不如单线程loop循环的性能高。
  - 协成里面重要的是一个关键字await的理解，async表示其修饰的是协程任务即task，await表示的是当线程执行到这一句，此时该task在此处挂起，然后调度器去执行其他的task，当这个挂起的部分处理完，会调用回掉函数告诉调度器我已经执行完了，那么调度器就返回来处理这个task的余下语句。